{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "595e03c4-f917-481d-a184-7d5432cca4e1",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79c79beb-966d-404c-afae-55bfb4edf3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JMU7\\Anaconda3\\envs\\TDS\\lib\\site-packages\\dash\\_jupyter.py:31: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  _dash_comm = Comm(target_name=\"dash\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # For data manipulation\n",
    "import re  # For regular expressions\n",
    "from mlxtend.frequent_patterns import association_rules, apriori  # For association rule mining\n",
    "from dash import Dash, dcc, html, Input, Output, callback  # For creating the web application\n",
    "import numpy  # For array manipulation\n",
    "import pickle  # For serializing and deserializing Python objects\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca327b1-286b-4d92-a4f7-593b73f06543",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "317b80c5-841b-4629-b297-752fc5982b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset\n",
    "dataset_path = \"dummy dataset.csv\"\n",
    "\n",
    "# Read the dataset using pandas\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Preprocess text data\n",
    "df['lower_text'] = df['Text'].apply(lambda x: re.sub('\\W+', ' ', x.lower()))\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1757b836-a849-4459-9586-6184fd916bbb",
   "metadata": {},
   "source": [
    "## Data Processing for Association Rule Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689306e3-3d2a-40ca-8ece-d599a5b50fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform text data into transactions\n",
    "full_data = pd.DataFrame()\n",
    "for i in df['index']:\n",
    "    # Split text into words\n",
    "    l1 = df['lower_text'].iloc[i].split(' ')\n",
    "    temp = pd.DataFrame({'item': l1, 'index': i})\n",
    "    full_data = pd.concat([full_data, temp], axis=0)\n",
    "\n",
    "# Group by transaction and item, count occurrences\n",
    "transactions_str = full_data[(full_data['item'] != '')].groupby(['index', 'item'])['item'].count().reset_index(name='Count')\n",
    "\n",
    "# Pivot table to create a matrix representation of transactions\n",
    "my_basket = transactions_str.pivot_table(index='index', columns='item', values='Count', aggfunc='sum').fillna(0)\n",
    "\n",
    "# Function to encode data as 0 or 1\n",
    "def encode(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    if x >= 1:\n",
    "        return 1\n",
    "\n",
    "# Apply encoding function to the dataset\n",
    "my_basket_sets = my_basket.applymap(encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e97fd52-b3d0-4941-a2bd-172611e9bafa",
   "metadata": {},
   "source": [
    "## Association Rule Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0651379-a0c0-4ab0-85d0-a7c5fbeffc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Apriori algorithm to find frequent itemsets\n",
    "frequent_items = apriori(my_basket_sets, min_support=0.01, use_colnames=True)\n",
    "\n",
    "# Generate association rules from frequent itemsets\n",
    "rules = association_rules(frequent_items, metric=\"lift\", min_threshold=1)\n",
    "rules.sort_values('confidence', ascending=False, inplace=True)\n",
    "rules['antecedents_consequents'] = rules[['antecedents', 'consequents']].apply(lambda x: list(x['antecedents']) + list(x['consequents']), axis=1)\n",
    "\n",
    "# Save the generated rules to a file\n",
    "with open('association_rules_model.pkl', 'wb') as file:\n",
    "    pickle.dump(rules, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8cc7eb-43ef-41eb-a6b9-021cd066916b",
   "metadata": {},
   "source": [
    "## Creating Dash Web Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16b5c173-9425-4225-a8e1-256fe59ba57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2acdb721eb0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "external_stylesheets = [\"https://codepen.io/chriddyp/pen/bWLwgP.css\"]\n",
    "\n",
    "# Load the saved model\n",
    "with open('association_rules_model.pkl', 'rb') as file:\n",
    "    saved_rules = pickle.load(file)\n",
    "\n",
    "app = Dash(__name__, external_stylesheets=external_stylesheets)\n",
    "app.layout = html.Div(\n",
    "    [\n",
    "        html.H1(\"Next Word Prediction using Apriori Algorithm\"),\n",
    "        html.I(\"Enter you input here :\"),\n",
    "        html.Br(),\n",
    "        dcc.Input(id=\"input1\", type=\"text\", placeholder=\"Sentence\", style={'marginRight':'10px','width':'90%'}),\n",
    "        html.Br(),html.Br(),\n",
    "        html.Label(\"Predicted next words :\"),\n",
    "        html.Div(id=\"output\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "@callback(\n",
    "    Output(\"output\", \"children\"),\n",
    "    Input(\"input1\", \"value\"),\n",
    ")\n",
    "def update_output(input1):\n",
    "    # Assuming 'new_antecedents' contains the antecedents for prediction\n",
    "    if input1 is None:\n",
    "        recommendation_list = ''\n",
    "        \n",
    "    else : \n",
    "        new_antecedents = input1.lower().split(' ') \n",
    "\n",
    "        # Predict consequents for the new antecedents\n",
    "        predicted_consequents = saved_rules[saved_rules['antecedents'].apply(lambda x: set(x).issuperset(new_antecedents))]\n",
    "        \n",
    "        temp1 = predicted_consequents.sort_values('confidence', ascending=False).head(20)\n",
    "        temp1['Recommendations'] = temp1[['antecedents_consequents']].apply(lambda x : [i for i in list(x['antecedents_consequents']) if i not in new_antecedents], axis = 1)\n",
    "\n",
    "        try : \n",
    "            recommendation_list = set(list(numpy.concatenate(list(temp1['Recommendations'])).flat))\n",
    "        except:\n",
    "            recommendation_list = ''\n",
    "    return f'{recommendation_list}'\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0034cd6f-0193-4c21-adf4-d63f1ae0f704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
